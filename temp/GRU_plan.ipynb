{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "import syft as sy\n",
    "from syft.execution.plan import Plan\n",
    "\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGruCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A forward only GRU cell.\n",
    "    Input should be: (sequence length x batch size x input_size).\n",
    "    The output is the output of the final forward call.\n",
    "    It's not clear if it would be possible to use the output from each cell in a Plan\n",
    "    because of the assumptions of 2D tensors in backprop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(CustomGruCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Reset Gate\n",
    "        self.fc_ir = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.fc_hr = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        # Update Gate\n",
    "        self.fc_iz = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.fc_hz = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        # New Gate\n",
    "        self.fc_in = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.fc_hn = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        i_r = self.fc_ir(x)\n",
    "        h_r = self.fc_hr(h)\n",
    "        i_z = self.fc_iz(x)\n",
    "        h_z = self.fc_hz(h)\n",
    "        i_n = self.fc_in(x)\n",
    "        h_n = self.fc_hn(h)\n",
    "\n",
    "        # Activation functions need to be on the object (not functional)\n",
    "        # for PySyft gradient stuff to work.\n",
    "        resetgate = (i_r + h_r).sigmoid()\n",
    "        inputgate = (i_z + h_z).sigmoid()\n",
    "        newgate = (i_n + (resetgate * h_n)).tanh()\n",
    "\n",
    "        hy = newgate + inputgate * (h - newgate)\n",
    "\n",
    "        return hy\n",
    "\n",
    "\n",
    "class CustomGru(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(CustomGru, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru_cell = CustomGruCell(input_size, hidden_size, bias)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return th.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden=None, sequence_length=None):\n",
    "        if hidden is None:\n",
    "            batch_size = x.shape[1]\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "        if sequence_length is None:\n",
    "            sequence_length = x.shape[0]\n",
    "        else:\n",
    "            # The sequence length should always be the same size when running the Plan.\n",
    "            # But maybe we can one day be more dynamic and use it.\n",
    "            sequence_length = sequence_length.item()\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            # `x.select(0, t)` == `x[t, :, :]` but it can be converted to Tensorflow.js.\n",
    "            hidden = self.gru_cell(x.select(0, t), hidden)\n",
    "        # Just return the result of the final cell\n",
    "        # since some PySyft autograd features seem like they have issues with 3D tensors.\n",
    "        output = hidden\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, padding_idx=0):\n",
    "        super(Net, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        # It would be nice use PyTorch's nn.Embedding and let them be trainable:\n",
    "        # `self.encoder = nn.Embedding(self.vocab_size, embedding_size)`\n",
    "        # but gradients didn't get computed for its weights when building the Plan.\n",
    "        # We could make our own custom embedding layer but it didn't work yet in PySyft:\n",
    "        # * Doing\n",
    "        #   `output = self.weight[embedding_indices]`\n",
    "        #   doesn't work with PySyft's implementation for __getitem__\n",
    "        #   because the indices have too many dimensions.\n",
    "        # * Doing the lookup \"manually\" with loops gave grad=None for the embeddings too.\n",
    "        embeddings = th.zeros(self.vocab_size, embedding_size)\n",
    "        self.encoder: nn.Embedding = nn.Embedding.from_pretrained(\n",
    "            embeddings, padding_idx=self.padding_idx\n",
    "        )\n",
    "        self.encoder.reset_parameters()\n",
    "\n",
    "        self.rnn = CustomGru(embedding_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, self.vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return self.rnn.init_hidden(batch_size)\n",
    "\n",
    "    def forward(self, x, hidden=None, sequence_length=None):\n",
    "        embeddings = self.encoder(x)\n",
    "        output, hidden = self.rnn(embeddings, hidden, sequence_length)\n",
    "        output = self.decoder(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_params(module, params_list, start_param_idx=0):\n",
    "    \"\"\"\n",
    "    Set params list into model recursively.\n",
    "    \"\"\"\n",
    "    param_idx = start_param_idx\n",
    "\n",
    "    for name, param in module._parameters.items():\n",
    "#         print('param name', name)\n",
    "        # A param can be None if it is not trainable.\n",
    "        if param is not None:\n",
    "            module._parameters[name] = params_list[param_idx]\n",
    "            param_idx += 1\n",
    "\n",
    "    for name, child in module._modules.items():\n",
    "#         print('module name', name)\n",
    "        if child is not None:\n",
    "            param_idx = set_model_params(child, params_list, param_idx)\n",
    "\n",
    "    return param_idx\n",
    "\n",
    "\n",
    "def softmax_cross_entropy_with_logits(logits, targets, batch_size):\n",
    "    # numstable logsoftmax\n",
    "    norm_logits = logits - logits.max()\n",
    "    log_probs = norm_logits - norm_logits.exp().sum(dim=1, keepdim=True).log()\n",
    "    return -(targets * log_probs).sum() / batch_size\n",
    "\n",
    "\n",
    "def naive_sgd(param, **kwargs):\n",
    "    if param.grad is None:\n",
    "        # A grad can be None if you used operations that are not supported\n",
    "        # by PySyft's autograd features or the param\n",
    "        # isn't trainable (e.g. nn.Embedding was used).\n",
    "        return param\n",
    "    return param - kwargs[\"lr\"] * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(10, 8, 8)\n",
    "\n",
    "@sy.func2plan()\n",
    "def train(data, initial_hidden, targets, lr, batch_size, sequence_length, model_parameters):\n",
    "    set_model_params(model, model_parameters)\n",
    "\n",
    "    logits, hidden = model(data, initial_hidden, sequence_length)\n",
    "\n",
    "    loss = softmax_cross_entropy_with_logits(logits, targets, batch_size)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "#     num_none_grads = len(list(filter(lambda param: param.grad is None, model_parameters)))\n",
    "#     # Only the grad for the embeddings will be None.\n",
    "#     assert (\n",
    "#         num_none_grads == 1\n",
    "#     ), f\"{num_none_grads}/{len(model_parameters)} model params have None grad(s).\"\n",
    "#     assert model_parameters[0].grad is None, \"The grad for the embeddings should be None.\"\n",
    "\n",
    "    updated_params = [naive_sgd(param, lr=lr) for param in model_parameters]\n",
    "\n",
    "    pred = th.argmax(logits, dim=1)\n",
    "    targets_idx = th.argmax(targets, dim=1)\n",
    "    acc = pred.eq(targets_idx).sum().float() / batch_size\n",
    "\n",
    "    return (loss, acc, *updated_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args (AutogradTensor>PlaceHolder[Id:18923778755]>tensor([[8, 4, 4],\n",
      "        [5, 6, 9]]), AutogradTensor>PlaceHolder[Id:63223660554]>tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]]), AutogradTensor>PlaceHolder[Id:88664011513]>tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), AutogradTensor>PlaceHolder[Id:14238578293]>tensor([0.1000]), AutogradTensor>PlaceHolder[Id:60659543254]>tensor([3]), AutogradTensor>PlaceHolder[Id:75285626848]>tensor([2]), [AutogradTensor>PlaceHolder[Id:45024693576]>Parameter containing:\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.5972e-01, -1.2182e+00,  4.6056e-01, -1.0211e-01,  2.0212e+00,\n",
      "          7.4920e-01,  2.2674e-01,  9.6931e-01],\n",
      "        [-1.4089e+00, -1.1746e+00, -4.7955e-02,  9.7812e-01,  8.0935e-01,\n",
      "          5.9334e-01, -1.1394e+00,  5.0496e-01],\n",
      "        [-2.9693e-03, -7.3615e-01,  5.0082e-01, -6.3128e-01,  1.1082e+00,\n",
      "         -7.7667e-01, -3.7083e-03, -1.6194e+00],\n",
      "        [ 5.2363e-01,  1.3347e+00, -1.2477e+00, -1.6576e-01,  1.0904e+00,\n",
      "          5.7500e-01,  4.6973e-01, -3.3326e-01],\n",
      "        [ 1.7072e+00, -1.2410e+00,  1.7996e-01, -9.0139e-01, -9.1930e-01,\n",
      "         -3.3625e-01,  4.5076e-01, -1.4341e+00],\n",
      "        [-1.1767e+00, -1.0370e+00, -6.1073e-01,  6.7230e-01, -1.5012e+00,\n",
      "         -1.4925e+00, -2.9079e-01,  1.1396e+00],\n",
      "        [ 8.7493e-01,  1.0622e+00,  1.1213e+00, -1.2905e-03,  2.1941e-01,\n",
      "          1.2460e+00, -1.5206e+00, -1.3818e+00],\n",
      "        [-5.7781e-01, -2.7995e-01,  3.4739e+00, -3.0765e-01,  3.6500e-01,\n",
      "          8.0277e-01,  8.0012e-02, -6.3106e-01],\n",
      "        [ 7.4035e-01, -6.3348e-01,  1.1349e-02,  5.0981e-01, -6.3492e-02,\n",
      "         -1.1296e+00, -1.7582e-01,  1.2976e-01]]), AutogradTensor>PlaceHolder[Id:990784360]>Parameter containing:\n",
      "tensor([[-0.0640, -0.2803, -0.1169, -0.1771, -0.2738,  0.2182, -0.0646,  0.0086],\n",
      "        [-0.3270,  0.0576,  0.0647,  0.0044,  0.3447, -0.2678, -0.1108,  0.1008],\n",
      "        [ 0.0721, -0.1691, -0.1494, -0.0703, -0.0300, -0.1352, -0.1312, -0.2950],\n",
      "        [-0.2500,  0.1414, -0.2799, -0.0097, -0.0281, -0.0959, -0.1393, -0.2253],\n",
      "        [-0.1205,  0.2942, -0.2287,  0.3236,  0.3375, -0.1774,  0.1308,  0.1011],\n",
      "        [-0.1115,  0.0430,  0.1932,  0.1495,  0.2103,  0.0914, -0.0169,  0.3438],\n",
      "        [-0.2909, -0.2891,  0.3321,  0.0513,  0.1740,  0.0673,  0.0970,  0.1848],\n",
      "        [ 0.3146,  0.2457, -0.0100, -0.0611,  0.0775,  0.1798,  0.1190, -0.0533]],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:47298712224]>Parameter containing:\n",
      "tensor([ 0.1791,  0.1394,  0.2304, -0.1425, -0.1426, -0.1694,  0.0342,  0.2975],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:62292024267]>Parameter containing:\n",
      "tensor([[ 0.0627, -0.3377,  0.0113,  0.1647, -0.0186,  0.3158,  0.1683,  0.0401],\n",
      "        [-0.0237, -0.1825,  0.0217,  0.1100,  0.0220, -0.1714,  0.1236, -0.1306],\n",
      "        [ 0.1853, -0.2793, -0.2244,  0.1890, -0.2160,  0.0402, -0.1398, -0.0939],\n",
      "        [ 0.3074,  0.2779, -0.2308,  0.2310,  0.2666,  0.1310, -0.3444, -0.2078],\n",
      "        [ 0.3126,  0.3029, -0.0156, -0.2198,  0.1957, -0.2400,  0.1833,  0.0380],\n",
      "        [ 0.2960,  0.0808,  0.0893, -0.2601, -0.0788,  0.1818,  0.3353,  0.2085],\n",
      "        [ 0.0749, -0.2764,  0.2986, -0.1671,  0.3221, -0.1016, -0.1177, -0.0537],\n",
      "        [ 0.2221, -0.1791,  0.1313, -0.2544, -0.2768, -0.0721, -0.2976, -0.2460]],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:43655452902]>Parameter containing:\n",
      "tensor([ 0.2910,  0.1481,  0.1529,  0.3313,  0.1567,  0.0253,  0.2175, -0.0574],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:4052953431]>Parameter containing:\n",
      "tensor([[ 0.0643, -0.3388,  0.0345, -0.1935,  0.0094, -0.0298, -0.1166,  0.2871],\n",
      "        [-0.1383,  0.1466,  0.2303, -0.3214,  0.1584, -0.2307,  0.2081, -0.0618],\n",
      "        [ 0.2955, -0.2462,  0.1048, -0.0886, -0.3224, -0.0053,  0.3093, -0.1587],\n",
      "        [ 0.1553,  0.1115,  0.2196, -0.2361, -0.0250, -0.2046,  0.0979, -0.0106],\n",
      "        [ 0.1704,  0.1278, -0.0040,  0.3397,  0.2197, -0.1902, -0.1619, -0.2039],\n",
      "        [-0.2816, -0.1233,  0.0480, -0.1078,  0.1304, -0.1718,  0.2328,  0.2688],\n",
      "        [ 0.2916,  0.2781,  0.1628, -0.0337,  0.2848,  0.3348,  0.1588, -0.2430],\n",
      "        [-0.1502, -0.2213, -0.2955,  0.1549,  0.1899, -0.2464,  0.1521,  0.1635]],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:4690589074]>Parameter containing:\n",
      "tensor([-0.1183, -0.2204, -0.2840, -0.1214,  0.1506, -0.1574,  0.3440,  0.3071],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:53817145924]>Parameter containing:\n",
      "tensor([[-0.3526, -0.2067, -0.1782,  0.0480,  0.2467,  0.2852, -0.0793,  0.1154],\n",
      "        [-0.2568, -0.0572, -0.2413, -0.3410,  0.2897,  0.2717, -0.1146, -0.0352],\n",
      "        [-0.2637,  0.0146, -0.0371, -0.1870,  0.1404, -0.0224, -0.0542,  0.0678],\n",
      "        [-0.0397, -0.2568,  0.1396,  0.0303, -0.2648,  0.3055,  0.1535,  0.1423],\n",
      "        [ 0.0811, -0.3354, -0.1353,  0.3054,  0.3085, -0.3301, -0.0451, -0.1504],\n",
      "        [ 0.3037, -0.1871, -0.2285,  0.3055,  0.0622, -0.1697,  0.3002,  0.3186],\n",
      "        [ 0.3488, -0.2415, -0.3219, -0.3081, -0.3254,  0.1276, -0.0810, -0.2966],\n",
      "        [ 0.0346,  0.1238,  0.1935,  0.0525, -0.3487,  0.1513, -0.1696,  0.2600]],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:440403048]>Parameter containing:\n",
      "tensor([ 0.0279, -0.3486, -0.1787, -0.0467,  0.0070, -0.1513,  0.0844, -0.2872],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:71714220449]>Parameter containing:\n",
      "tensor([[ 0.2800,  0.1564,  0.2779, -0.0744, -0.1064,  0.0812, -0.0130,  0.0320],\n",
      "        [-0.1723, -0.2736, -0.0821, -0.3129, -0.2555, -0.1514,  0.1116,  0.3056],\n",
      "        [ 0.2182, -0.1260, -0.3240,  0.0572, -0.1289,  0.1605, -0.0263, -0.3056],\n",
      "        [ 0.2286,  0.2307, -0.3496,  0.2594,  0.0555, -0.0071,  0.0583,  0.1130],\n",
      "        [-0.3216, -0.0941,  0.0666,  0.3507, -0.3139,  0.1698, -0.0263, -0.3221],\n",
      "        [ 0.1341, -0.3126, -0.3491, -0.1546, -0.1929,  0.1803, -0.1509, -0.3250],\n",
      "        [ 0.3054,  0.2761, -0.1910, -0.0954,  0.2200,  0.0598, -0.2855, -0.2234],\n",
      "        [ 0.0833,  0.2964, -0.1620, -0.2503,  0.0438,  0.3243, -0.2283, -0.3316]],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:12802414789]>Parameter containing:\n",
      "tensor([ 0.1810, -0.1931,  0.0931,  0.1774,  0.0982,  0.3446, -0.0916,  0.2811],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:59522770156]>Parameter containing:\n",
      "tensor([[-0.1378,  0.0767,  0.1069,  0.1365, -0.2600,  0.1562,  0.2425,  0.2785],\n",
      "        [-0.1411, -0.0803, -0.3047, -0.1685, -0.0392,  0.0346, -0.0163, -0.1971],\n",
      "        [-0.0568, -0.1110,  0.2076, -0.0236,  0.2166,  0.1866,  0.1132,  0.1054],\n",
      "        [-0.1818,  0.2261, -0.2849,  0.0893, -0.0083,  0.1690, -0.1024, -0.0735],\n",
      "        [ 0.0789,  0.0424,  0.0148,  0.1828,  0.2216,  0.3154,  0.0197,  0.1145],\n",
      "        [ 0.2434, -0.1567,  0.2273, -0.2185, -0.0703,  0.0890,  0.0947, -0.3475],\n",
      "        [ 0.1658,  0.3233, -0.3532, -0.1023, -0.1436,  0.1028, -0.1909,  0.0620],\n",
      "        [ 0.2984, -0.1163,  0.2729, -0.1239, -0.3492,  0.3076, -0.0620,  0.0458]],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:86519257217]>Parameter containing:\n",
      "tensor([ 0.1526,  0.2996,  0.1621, -0.0625,  0.2079,  0.3434,  0.2610,  0.3178],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:46869739010]>Parameter containing:\n",
      "tensor([[ 0.2727,  0.2505,  0.1742, -0.2800,  0.2602, -0.3197, -0.3503, -0.1412],\n",
      "        [ 0.2821, -0.1795,  0.3027,  0.0412, -0.3496, -0.3376, -0.0243, -0.1046],\n",
      "        [-0.2696, -0.2286, -0.2255, -0.1420,  0.0968,  0.2065, -0.3113, -0.1089],\n",
      "        [ 0.2804, -0.0407, -0.1119, -0.3453,  0.0276, -0.3167, -0.2662,  0.1667],\n",
      "        [ 0.0116,  0.3311,  0.2072, -0.0106,  0.1119, -0.0981,  0.1375,  0.2512],\n",
      "        [ 0.2416, -0.3024, -0.2511, -0.0635, -0.3417, -0.0463, -0.2231, -0.0519],\n",
      "        [-0.0477, -0.0959,  0.0467, -0.1481,  0.3319, -0.1454,  0.1834,  0.3024],\n",
      "        [ 0.2955,  0.1847, -0.0369,  0.3471, -0.3244,  0.1577,  0.0639,  0.3116],\n",
      "        [-0.2878,  0.1671, -0.1442, -0.2357,  0.1274,  0.0079,  0.0297,  0.1686],\n",
      "        [ 0.0604,  0.0432,  0.2615, -0.0447,  0.0687, -0.2421, -0.1766,  0.1676]],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:10493099134]>Parameter containing:\n",
      "tensor([-0.1002,  0.3415,  0.0322,  0.2834,  0.0688, -0.2701,  0.0852, -0.0853,\n",
      "         0.3187,  0.1568], requires_grad=True)])\n",
      "Framework args {}\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:29485908111]>tensor([[-0.5778, -0.2800,  3.4739, -0.3077,  0.3650,  0.8028,  0.0800, -0.6311],\n",
      "        [ 0.5236,  1.3347, -1.2477, -0.1658,  1.0904,  0.5750,  0.4697, -0.3333],\n",
      "        [ 0.5236,  1.3347, -1.2477, -0.1658,  1.0904,  0.5750,  0.4697, -0.3333]]) AutogradTensor>PlaceHolder[Id:990784360]>Parameter containing:\n",
      "tensor([[-0.0640, -0.2803, -0.1169, -0.1771, -0.2738,  0.2182, -0.0646,  0.0086],\n",
      "        [-0.3270,  0.0576,  0.0647,  0.0044,  0.3447, -0.2678, -0.1108,  0.1008],\n",
      "        [ 0.0721, -0.1691, -0.1494, -0.0703, -0.0300, -0.1352, -0.1312, -0.2950],\n",
      "        [-0.2500,  0.1414, -0.2799, -0.0097, -0.0281, -0.0959, -0.1393, -0.2253],\n",
      "        [-0.1205,  0.2942, -0.2287,  0.3236,  0.3375, -0.1774,  0.1308,  0.1011],\n",
      "        [-0.1115,  0.0430,  0.1932,  0.1495,  0.2103,  0.0914, -0.0169,  0.3438],\n",
      "        [-0.2909, -0.2891,  0.3321,  0.0513,  0.1740,  0.0673,  0.0970,  0.1848],\n",
      "        [ 0.3146,  0.2457, -0.0100, -0.0611,  0.0775,  0.1798,  0.1190, -0.0533]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:47298712224]>Parameter containing:\n",
      "tensor([ 0.1791,  0.1394,  0.2304, -0.1425, -0.1426, -0.1694,  0.0342,  0.2975],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:63223660554]>tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]]) AutogradTensor>PlaceHolder[Id:62292024267]>Parameter containing:\n",
      "tensor([[ 0.0627, -0.3377,  0.0113,  0.1647, -0.0186,  0.3158,  0.1683,  0.0401],\n",
      "        [-0.0237, -0.1825,  0.0217,  0.1100,  0.0220, -0.1714,  0.1236, -0.1306],\n",
      "        [ 0.1853, -0.2793, -0.2244,  0.1890, -0.2160,  0.0402, -0.1398, -0.0939],\n",
      "        [ 0.3074,  0.2779, -0.2308,  0.2310,  0.2666,  0.1310, -0.3444, -0.2078],\n",
      "        [ 0.3126,  0.3029, -0.0156, -0.2198,  0.1957, -0.2400,  0.1833,  0.0380],\n",
      "        [ 0.2960,  0.0808,  0.0893, -0.2601, -0.0788,  0.1818,  0.3353,  0.2085],\n",
      "        [ 0.0749, -0.2764,  0.2986, -0.1671,  0.3221, -0.1016, -0.1177, -0.0537],\n",
      "        [ 0.2221, -0.1791,  0.1313, -0.2544, -0.2768, -0.0721, -0.2976, -0.2460]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:43655452902]>Parameter containing:\n",
      "tensor([ 0.2910,  0.1481,  0.1529,  0.3313,  0.1567,  0.0253,  0.2175, -0.0574],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:29485908111]>tensor([[-0.5778, -0.2800,  3.4739, -0.3077,  0.3650,  0.8028,  0.0800, -0.6311],\n",
      "        [ 0.5236,  1.3347, -1.2477, -0.1658,  1.0904,  0.5750,  0.4697, -0.3333],\n",
      "        [ 0.5236,  1.3347, -1.2477, -0.1658,  1.0904,  0.5750,  0.4697, -0.3333]]) AutogradTensor>PlaceHolder[Id:4052953431]>Parameter containing:\n",
      "tensor([[ 0.0643, -0.3388,  0.0345, -0.1935,  0.0094, -0.0298, -0.1166,  0.2871],\n",
      "        [-0.1383,  0.1466,  0.2303, -0.3214,  0.1584, -0.2307,  0.2081, -0.0618],\n",
      "        [ 0.2955, -0.2462,  0.1048, -0.0886, -0.3224, -0.0053,  0.3093, -0.1587],\n",
      "        [ 0.1553,  0.1115,  0.2196, -0.2361, -0.0250, -0.2046,  0.0979, -0.0106],\n",
      "        [ 0.1704,  0.1278, -0.0040,  0.3397,  0.2197, -0.1902, -0.1619, -0.2039],\n",
      "        [-0.2816, -0.1233,  0.0480, -0.1078,  0.1304, -0.1718,  0.2328,  0.2688],\n",
      "        [ 0.2916,  0.2781,  0.1628, -0.0337,  0.2848,  0.3348,  0.1588, -0.2430],\n",
      "        [-0.1502, -0.2213, -0.2955,  0.1549,  0.1899, -0.2464,  0.1521,  0.1635]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:4690589074]>Parameter containing:\n",
      "tensor([-0.1183, -0.2204, -0.2840, -0.1214,  0.1506, -0.1574,  0.3440,  0.3071],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:63223660554]>tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]]) AutogradTensor>PlaceHolder[Id:53817145924]>Parameter containing:\n",
      "tensor([[-0.3526, -0.2067, -0.1782,  0.0480,  0.2467,  0.2852, -0.0793,  0.1154],\n",
      "        [-0.2568, -0.0572, -0.2413, -0.3410,  0.2897,  0.2717, -0.1146, -0.0352],\n",
      "        [-0.2637,  0.0146, -0.0371, -0.1870,  0.1404, -0.0224, -0.0542,  0.0678],\n",
      "        [-0.0397, -0.2568,  0.1396,  0.0303, -0.2648,  0.3055,  0.1535,  0.1423],\n",
      "        [ 0.0811, -0.3354, -0.1353,  0.3054,  0.3085, -0.3301, -0.0451, -0.1504],\n",
      "        [ 0.3037, -0.1871, -0.2285,  0.3055,  0.0622, -0.1697,  0.3002,  0.3186],\n",
      "        [ 0.3488, -0.2415, -0.3219, -0.3081, -0.3254,  0.1276, -0.0810, -0.2966],\n",
      "        [ 0.0346,  0.1238,  0.1935,  0.0525, -0.3487,  0.1513, -0.1696,  0.2600]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:440403048]>Parameter containing:\n",
      "tensor([ 0.0279, -0.3486, -0.1787, -0.0467,  0.0070, -0.1513,  0.0844, -0.2872],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:29485908111]>tensor([[-0.5778, -0.2800,  3.4739, -0.3077,  0.3650,  0.8028,  0.0800, -0.6311],\n",
      "        [ 0.5236,  1.3347, -1.2477, -0.1658,  1.0904,  0.5750,  0.4697, -0.3333],\n",
      "        [ 0.5236,  1.3347, -1.2477, -0.1658,  1.0904,  0.5750,  0.4697, -0.3333]]) AutogradTensor>PlaceHolder[Id:71714220449]>Parameter containing:\n",
      "tensor([[ 0.2800,  0.1564,  0.2779, -0.0744, -0.1064,  0.0812, -0.0130,  0.0320],\n",
      "        [-0.1723, -0.2736, -0.0821, -0.3129, -0.2555, -0.1514,  0.1116,  0.3056],\n",
      "        [ 0.2182, -0.1260, -0.3240,  0.0572, -0.1289,  0.1605, -0.0263, -0.3056],\n",
      "        [ 0.2286,  0.2307, -0.3496,  0.2594,  0.0555, -0.0071,  0.0583,  0.1130],\n",
      "        [-0.3216, -0.0941,  0.0666,  0.3507, -0.3139,  0.1698, -0.0263, -0.3221],\n",
      "        [ 0.1341, -0.3126, -0.3491, -0.1546, -0.1929,  0.1803, -0.1509, -0.3250],\n",
      "        [ 0.3054,  0.2761, -0.1910, -0.0954,  0.2200,  0.0598, -0.2855, -0.2234],\n",
      "        [ 0.0833,  0.2964, -0.1620, -0.2503,  0.0438,  0.3243, -0.2283, -0.3316]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:12802414789]>Parameter containing:\n",
      "tensor([ 0.1810, -0.1931,  0.0931,  0.1774,  0.0982,  0.3446, -0.0916,  0.2811],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:63223660554]>tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]]) AutogradTensor>PlaceHolder[Id:59522770156]>Parameter containing:\n",
      "tensor([[-0.1378,  0.0767,  0.1069,  0.1365, -0.2600,  0.1562,  0.2425,  0.2785],\n",
      "        [-0.1411, -0.0803, -0.3047, -0.1685, -0.0392,  0.0346, -0.0163, -0.1971],\n",
      "        [-0.0568, -0.1110,  0.2076, -0.0236,  0.2166,  0.1866,  0.1132,  0.1054],\n",
      "        [-0.1818,  0.2261, -0.2849,  0.0893, -0.0083,  0.1690, -0.1024, -0.0735],\n",
      "        [ 0.0789,  0.0424,  0.0148,  0.1828,  0.2216,  0.3154,  0.0197,  0.1145],\n",
      "        [ 0.2434, -0.1567,  0.2273, -0.2185, -0.0703,  0.0890,  0.0947, -0.3475],\n",
      "        [ 0.1658,  0.3233, -0.3532, -0.1023, -0.1436,  0.1028, -0.1909,  0.0620],\n",
      "        [ 0.2984, -0.1163,  0.2729, -0.1239, -0.3492,  0.3076, -0.0620,  0.0458]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:86519257217]>Parameter containing:\n",
      "tensor([ 0.1526,  0.2996,  0.1621, -0.0625,  0.2079,  0.3434,  0.2610,  0.3178],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:66525488765]>tensor([[ 1.7072, -1.2410,  0.1800, -0.9014, -0.9193, -0.3363,  0.4508, -1.4341],\n",
      "        [-1.1767, -1.0370, -0.6107,  0.6723, -1.5012, -1.4925, -0.2908,  1.1396],\n",
      "        [ 0.7403, -0.6335,  0.0113,  0.5098, -0.0635, -1.1296, -0.1758,  0.1298]]) AutogradTensor>PlaceHolder[Id:990784360]>Parameter containing:\n",
      "tensor([[-0.0640, -0.2803, -0.1169, -0.1771, -0.2738,  0.2182, -0.0646,  0.0086],\n",
      "        [-0.3270,  0.0576,  0.0647,  0.0044,  0.3447, -0.2678, -0.1108,  0.1008],\n",
      "        [ 0.0721, -0.1691, -0.1494, -0.0703, -0.0300, -0.1352, -0.1312, -0.2950],\n",
      "        [-0.2500,  0.1414, -0.2799, -0.0097, -0.0281, -0.0959, -0.1393, -0.2253],\n",
      "        [-0.1205,  0.2942, -0.2287,  0.3236,  0.3375, -0.1774,  0.1308,  0.1011],\n",
      "        [-0.1115,  0.0430,  0.1932,  0.1495,  0.2103,  0.0914, -0.0169,  0.3438],\n",
      "        [-0.2909, -0.2891,  0.3321,  0.0513,  0.1740,  0.0673,  0.0970,  0.1848],\n",
      "        [ 0.3146,  0.2457, -0.0100, -0.0611,  0.0775,  0.1798,  0.1190, -0.0533]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:47298712224]>Parameter containing:\n",
      "tensor([ 0.1791,  0.1394,  0.2304, -0.1425, -0.1426, -0.1694,  0.0342,  0.2975],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:1673661744]>tensor([[ 0.4048, -0.1679, -0.3569, -0.3568,  0.3145, -0.1725, -0.1016,  0.2236],\n",
      "        [ 0.1267, -0.3952,  0.3668,  0.4328, -0.1246,  0.3181,  0.1494,  0.4305],\n",
      "        [ 0.1267, -0.3952,  0.3668,  0.4328, -0.1246,  0.3181,  0.1494,  0.4305]],\n",
      "       grad_fn=<AddBackward0>) AutogradTensor>PlaceHolder[Id:62292024267]>Parameter containing:\n",
      "tensor([[ 0.0627, -0.3377,  0.0113,  0.1647, -0.0186,  0.3158,  0.1683,  0.0401],\n",
      "        [-0.0237, -0.1825,  0.0217,  0.1100,  0.0220, -0.1714,  0.1236, -0.1306],\n",
      "        [ 0.1853, -0.2793, -0.2244,  0.1890, -0.2160,  0.0402, -0.1398, -0.0939],\n",
      "        [ 0.3074,  0.2779, -0.2308,  0.2310,  0.2666,  0.1310, -0.3444, -0.2078],\n",
      "        [ 0.3126,  0.3029, -0.0156, -0.2198,  0.1957, -0.2400,  0.1833,  0.0380],\n",
      "        [ 0.2960,  0.0808,  0.0893, -0.2601, -0.0788,  0.1818,  0.3353,  0.2085],\n",
      "        [ 0.0749, -0.2764,  0.2986, -0.1671,  0.3221, -0.1016, -0.1177, -0.0537],\n",
      "        [ 0.2221, -0.1791,  0.1313, -0.2544, -0.2768, -0.0721, -0.2976, -0.2460]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:43655452902]>Parameter containing:\n",
      "tensor([ 0.2910,  0.1481,  0.1529,  0.3313,  0.1567,  0.0253,  0.2175, -0.0574],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:66525488765]>tensor([[ 1.7072, -1.2410,  0.1800, -0.9014, -0.9193, -0.3363,  0.4508, -1.4341],\n",
      "        [-1.1767, -1.0370, -0.6107,  0.6723, -1.5012, -1.4925, -0.2908,  1.1396],\n",
      "        [ 0.7403, -0.6335,  0.0113,  0.5098, -0.0635, -1.1296, -0.1758,  0.1298]]) AutogradTensor>PlaceHolder[Id:4052953431]>Parameter containing:\n",
      "tensor([[ 0.0643, -0.3388,  0.0345, -0.1935,  0.0094, -0.0298, -0.1166,  0.2871],\n",
      "        [-0.1383,  0.1466,  0.2303, -0.3214,  0.1584, -0.2307,  0.2081, -0.0618],\n",
      "        [ 0.2955, -0.2462,  0.1048, -0.0886, -0.3224, -0.0053,  0.3093, -0.1587],\n",
      "        [ 0.1553,  0.1115,  0.2196, -0.2361, -0.0250, -0.2046,  0.0979, -0.0106],\n",
      "        [ 0.1704,  0.1278, -0.0040,  0.3397,  0.2197, -0.1902, -0.1619, -0.2039],\n",
      "        [-0.2816, -0.1233,  0.0480, -0.1078,  0.1304, -0.1718,  0.2328,  0.2688],\n",
      "        [ 0.2916,  0.2781,  0.1628, -0.0337,  0.2848,  0.3348,  0.1588, -0.2430],\n",
      "        [-0.1502, -0.2213, -0.2955,  0.1549,  0.1899, -0.2464,  0.1521,  0.1635]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:4690589074]>Parameter containing:\n",
      "tensor([-0.1183, -0.2204, -0.2840, -0.1214,  0.1506, -0.1574,  0.3440,  0.3071],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:1673661744]>tensor([[ 0.4048, -0.1679, -0.3569, -0.3568,  0.3145, -0.1725, -0.1016,  0.2236],\n",
      "        [ 0.1267, -0.3952,  0.3668,  0.4328, -0.1246,  0.3181,  0.1494,  0.4305],\n",
      "        [ 0.1267, -0.3952,  0.3668,  0.4328, -0.1246,  0.3181,  0.1494,  0.4305]],\n",
      "       grad_fn=<AddBackward0>) AutogradTensor>PlaceHolder[Id:53817145924]>Parameter containing:\n",
      "tensor([[-0.3526, -0.2067, -0.1782,  0.0480,  0.2467,  0.2852, -0.0793,  0.1154],\n",
      "        [-0.2568, -0.0572, -0.2413, -0.3410,  0.2897,  0.2717, -0.1146, -0.0352],\n",
      "        [-0.2637,  0.0146, -0.0371, -0.1870,  0.1404, -0.0224, -0.0542,  0.0678],\n",
      "        [-0.0397, -0.2568,  0.1396,  0.0303, -0.2648,  0.3055,  0.1535,  0.1423],\n",
      "        [ 0.0811, -0.3354, -0.1353,  0.3054,  0.3085, -0.3301, -0.0451, -0.1504],\n",
      "        [ 0.3037, -0.1871, -0.2285,  0.3055,  0.0622, -0.1697,  0.3002,  0.3186],\n",
      "        [ 0.3488, -0.2415, -0.3219, -0.3081, -0.3254,  0.1276, -0.0810, -0.2966],\n",
      "        [ 0.0346,  0.1238,  0.1935,  0.0525, -0.3487,  0.1513, -0.1696,  0.2600]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:440403048]>Parameter containing:\n",
      "tensor([ 0.0279, -0.3486, -0.1787, -0.0467,  0.0070, -0.1513,  0.0844, -0.2872],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:66525488765]>tensor([[ 1.7072, -1.2410,  0.1800, -0.9014, -0.9193, -0.3363,  0.4508, -1.4341],\n",
      "        [-1.1767, -1.0370, -0.6107,  0.6723, -1.5012, -1.4925, -0.2908,  1.1396],\n",
      "        [ 0.7403, -0.6335,  0.0113,  0.5098, -0.0635, -1.1296, -0.1758,  0.1298]]) AutogradTensor>PlaceHolder[Id:71714220449]>Parameter containing:\n",
      "tensor([[ 0.2800,  0.1564,  0.2779, -0.0744, -0.1064,  0.0812, -0.0130,  0.0320],\n",
      "        [-0.1723, -0.2736, -0.0821, -0.3129, -0.2555, -0.1514,  0.1116,  0.3056],\n",
      "        [ 0.2182, -0.1260, -0.3240,  0.0572, -0.1289,  0.1605, -0.0263, -0.3056],\n",
      "        [ 0.2286,  0.2307, -0.3496,  0.2594,  0.0555, -0.0071,  0.0583,  0.1130],\n",
      "        [-0.3216, -0.0941,  0.0666,  0.3507, -0.3139,  0.1698, -0.0263, -0.3221],\n",
      "        [ 0.1341, -0.3126, -0.3491, -0.1546, -0.1929,  0.1803, -0.1509, -0.3250],\n",
      "        [ 0.3054,  0.2761, -0.1910, -0.0954,  0.2200,  0.0598, -0.2855, -0.2234],\n",
      "        [ 0.0833,  0.2964, -0.1620, -0.2503,  0.0438,  0.3243, -0.2283, -0.3316]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:12802414789]>Parameter containing:\n",
      "tensor([ 0.1810, -0.1931,  0.0931,  0.1774,  0.0982,  0.3446, -0.0916,  0.2811],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:1673661744]>tensor([[ 0.4048, -0.1679, -0.3569, -0.3568,  0.3145, -0.1725, -0.1016,  0.2236],\n",
      "        [ 0.1267, -0.3952,  0.3668,  0.4328, -0.1246,  0.3181,  0.1494,  0.4305],\n",
      "        [ 0.1267, -0.3952,  0.3668,  0.4328, -0.1246,  0.3181,  0.1494,  0.4305]],\n",
      "       grad_fn=<AddBackward0>) AutogradTensor>PlaceHolder[Id:59522770156]>Parameter containing:\n",
      "tensor([[-0.1378,  0.0767,  0.1069,  0.1365, -0.2600,  0.1562,  0.2425,  0.2785],\n",
      "        [-0.1411, -0.0803, -0.3047, -0.1685, -0.0392,  0.0346, -0.0163, -0.1971],\n",
      "        [-0.0568, -0.1110,  0.2076, -0.0236,  0.2166,  0.1866,  0.1132,  0.1054],\n",
      "        [-0.1818,  0.2261, -0.2849,  0.0893, -0.0083,  0.1690, -0.1024, -0.0735],\n",
      "        [ 0.0789,  0.0424,  0.0148,  0.1828,  0.2216,  0.3154,  0.0197,  0.1145],\n",
      "        [ 0.2434, -0.1567,  0.2273, -0.2185, -0.0703,  0.0890,  0.0947, -0.3475],\n",
      "        [ 0.1658,  0.3233, -0.3532, -0.1023, -0.1436,  0.1028, -0.1909,  0.0620],\n",
      "        [ 0.2984, -0.1163,  0.2729, -0.1239, -0.3492,  0.3076, -0.0620,  0.0458]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:86519257217]>Parameter containing:\n",
      "tensor([ 0.1526,  0.2996,  0.1621, -0.0625,  0.2079,  0.3434,  0.2610,  0.3178],\n",
      "       requires_grad=True)\n",
      "functional.linear AutogradTensor>PlaceHolder[Id:61373265994]>tensor([[ 0.4481,  0.0093, -0.0777, -0.3152,  0.2181,  0.5518,  0.0107,  0.4420],\n",
      "        [ 0.0393,  0.5156,  0.1608,  0.1885,  0.3062,  0.3620, -0.7017,  0.1581],\n",
      "        [ 0.2614, -0.1585,  0.3727,  0.3478, -0.0522,  0.3938, -0.0049,  0.2601]],\n",
      "       grad_fn=<AddBackward0>) AutogradTensor>PlaceHolder[Id:46869739010]>Parameter containing:\n",
      "tensor([[ 0.2727,  0.2505,  0.1742, -0.2800,  0.2602, -0.3197, -0.3503, -0.1412],\n",
      "        [ 0.2821, -0.1795,  0.3027,  0.0412, -0.3496, -0.3376, -0.0243, -0.1046],\n",
      "        [-0.2696, -0.2286, -0.2255, -0.1420,  0.0968,  0.2065, -0.3113, -0.1089],\n",
      "        [ 0.2804, -0.0407, -0.1119, -0.3453,  0.0276, -0.3167, -0.2662,  0.1667],\n",
      "        [ 0.0116,  0.3311,  0.2072, -0.0106,  0.1119, -0.0981,  0.1375,  0.2512],\n",
      "        [ 0.2416, -0.3024, -0.2511, -0.0635, -0.3417, -0.0463, -0.2231, -0.0519],\n",
      "        [-0.0477, -0.0959,  0.0467, -0.1481,  0.3319, -0.1454,  0.1834,  0.3024],\n",
      "        [ 0.2955,  0.1847, -0.0369,  0.3471, -0.3244,  0.1577,  0.0639,  0.3116],\n",
      "        [-0.2878,  0.1671, -0.1442, -0.2357,  0.1274,  0.0079,  0.0297,  0.1686],\n",
      "        [ 0.0604,  0.0432,  0.2615, -0.0447,  0.0687, -0.2421, -0.1766,  0.1676]],\n",
      "       requires_grad=True) AutogradTensor>PlaceHolder[Id:10493099134]>Parameter containing:\n",
      "tensor([-0.1002,  0.3415,  0.0322,  0.2834,  0.0688, -0.2701,  0.0852, -0.0853,\n",
      "         0.3187,  0.1568], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(AutogradTensor>PlaceHolder[Id:4560497702]>tensor([2.3363], grad_fn=<DivBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:6761244932]>tensor([0.]),\n",
       " AutogradTensor>PlaceHolder[Id:45024693576]>Parameter containing:\n",
       " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [-1.5972e-01, -1.2182e+00,  4.6056e-01, -1.0211e-01,  2.0212e+00,\n",
       "           7.4920e-01,  2.2674e-01,  9.6931e-01],\n",
       "         [-1.4089e+00, -1.1746e+00, -4.7955e-02,  9.7812e-01,  8.0935e-01,\n",
       "           5.9334e-01, -1.1394e+00,  5.0496e-01],\n",
       "         [-2.9693e-03, -7.3615e-01,  5.0082e-01, -6.3128e-01,  1.1082e+00,\n",
       "          -7.7667e-01, -3.7083e-03, -1.6194e+00],\n",
       "         [ 5.2363e-01,  1.3347e+00, -1.2477e+00, -1.6576e-01,  1.0904e+00,\n",
       "           5.7500e-01,  4.6973e-01, -3.3326e-01],\n",
       "         [ 1.7072e+00, -1.2410e+00,  1.7996e-01, -9.0139e-01, -9.1930e-01,\n",
       "          -3.3625e-01,  4.5076e-01, -1.4341e+00],\n",
       "         [-1.1767e+00, -1.0370e+00, -6.1073e-01,  6.7230e-01, -1.5012e+00,\n",
       "          -1.4925e+00, -2.9079e-01,  1.1396e+00],\n",
       "         [ 8.7493e-01,  1.0622e+00,  1.1213e+00, -1.2905e-03,  2.1941e-01,\n",
       "           1.2460e+00, -1.5206e+00, -1.3818e+00],\n",
       "         [-5.7781e-01, -2.7995e-01,  3.4739e+00, -3.0765e-01,  3.6500e-01,\n",
       "           8.0277e-01,  8.0012e-02, -6.3106e-01],\n",
       "         [ 7.4035e-01, -6.3348e-01,  1.1349e-02,  5.0981e-01, -6.3492e-02,\n",
       "          -1.1296e+00, -1.7582e-01,  1.2976e-01]]),\n",
       " AutogradTensor>PlaceHolder[Id:36017871971]>tensor([[-0.0643, -0.2803, -0.1169, -0.1773, -0.2740,  0.2185, -0.0646,  0.0086],\n",
       "         [-0.3277,  0.0580,  0.0644,  0.0047,  0.3449, -0.2677, -0.1110,  0.1014],\n",
       "         [ 0.0714, -0.1692, -0.1500, -0.0701, -0.0306, -0.1354, -0.1312, -0.2946],\n",
       "         [-0.2499,  0.1414, -0.2798, -0.0098, -0.0281, -0.0959, -0.1392, -0.2253],\n",
       "         [-0.1203,  0.2941, -0.2286,  0.3237,  0.3375, -0.1776,  0.1308,  0.1010],\n",
       "         [-0.1111,  0.0429,  0.1936,  0.1495,  0.2105,  0.0913, -0.0168,  0.3435],\n",
       "         [-0.2913, -0.2889,  0.3320,  0.0515,  0.1741,  0.0674,  0.0969,  0.1851],\n",
       "         [ 0.3141,  0.2462, -0.0108, -0.0610,  0.0776,  0.1800,  0.1189, -0.0529]],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:85409463475]>tensor([ 0.1785,  0.1388,  0.2303, -0.1425, -0.1422, -0.1688,  0.0339,  0.2967],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:80097145760]>tensor([[ 0.0627, -0.3375,  0.0111,  0.1645, -0.0185,  0.3156,  0.1682,  0.0399],\n",
       "         [-0.0239, -0.1824,  0.0218,  0.1101,  0.0219, -0.1714,  0.1237, -0.1307],\n",
       "         [ 0.1853, -0.2793, -0.2244,  0.1890, -0.2161,  0.0402, -0.1398, -0.0938],\n",
       "         [ 0.3074,  0.2779, -0.2308,  0.2310,  0.2666,  0.1310, -0.3444, -0.2078],\n",
       "         [ 0.3127,  0.3028, -0.0155, -0.2197,  0.1957, -0.2400,  0.1833,  0.0381],\n",
       "         [ 0.2961,  0.0807,  0.0894, -0.2600, -0.0788,  0.1819,  0.3354,  0.2087],\n",
       "         [ 0.0748, -0.2764,  0.2987, -0.1671,  0.3221, -0.1016, -0.1177, -0.0538],\n",
       "         [ 0.2219, -0.1789,  0.1312, -0.2545, -0.2769, -0.0722, -0.2976, -0.2462]],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:52224833131]>tensor([ 0.2903,  0.1475,  0.1529,  0.3313,  0.1571,  0.0259,  0.2172, -0.0582],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:1658178622]>tensor([[ 0.0647, -0.3397,  0.0392, -0.1938,  0.0100, -0.0295, -0.1164,  0.2857],\n",
       "         [-0.1369,  0.1451,  0.2299, -0.3215,  0.1574, -0.2319,  0.2081, -0.0623],\n",
       "         [ 0.2984, -0.2488,  0.1014, -0.0894, -0.3253, -0.0076,  0.3098, -0.1599],\n",
       "         [ 0.1553,  0.1115,  0.2180, -0.2359, -0.0254, -0.2051,  0.0978, -0.0102],\n",
       "         [ 0.1714,  0.1288, -0.0059,  0.3393,  0.2203, -0.1898, -0.1615, -0.2043],\n",
       "         [-0.2869, -0.1202,  0.0487, -0.1055,  0.1325, -0.1705,  0.2315,  0.2726],\n",
       "         [ 0.2932,  0.2788,  0.1607, -0.0347,  0.2851,  0.3355,  0.1594, -0.2441],\n",
       "         [-0.1500, -0.2211, -0.2958,  0.1541,  0.1896, -0.2458,  0.1527,  0.1627]],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:17794224573]>tensor([-0.1154, -0.2200, -0.2828, -0.1217,  0.1503, -0.1603,  0.3436,  0.3083],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:35539637276]>tensor([[-0.3524, -0.2071, -0.1779,  0.0483,  0.2467,  0.2855, -0.0792,  0.1158],\n",
       "         [-0.2565, -0.0575, -0.2413, -0.3410,  0.2898,  0.2717, -0.1146, -0.0349],\n",
       "         [-0.2629,  0.0141, -0.0375, -0.1874,  0.1409, -0.0225, -0.0543,  0.0685],\n",
       "         [-0.0397, -0.2568,  0.1396,  0.0304, -0.2648,  0.3055,  0.1535,  0.1423],\n",
       "         [ 0.0811, -0.3353, -0.1355,  0.3051,  0.3086, -0.3302, -0.0452, -0.1505],\n",
       "         [ 0.3026, -0.1865, -0.2277,  0.3063,  0.0614, -0.1693,  0.3004,  0.3178],\n",
       "         [ 0.3489, -0.2412, -0.3225, -0.3087, -0.3251,  0.1272, -0.0812, -0.2968],\n",
       "         [ 0.0348,  0.1238,  0.1932,  0.0522, -0.3485,  0.1511, -0.1697,  0.2600]],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:25915408497]>tensor([ 0.0308, -0.3482, -0.1774, -0.0470,  0.0066, -0.1543,  0.0840, -0.2860],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:39416967445]>tensor([[ 0.2687,  0.1581,  0.2781, -0.0718, -0.1081,  0.0836, -0.0163,  0.0395],\n",
       "         [-0.1850, -0.2649, -0.0857, -0.3109, -0.2523, -0.1448,  0.1098,  0.3129],\n",
       "         [ 0.2092, -0.1264, -0.3341,  0.0594, -0.1357,  0.1571, -0.0273, -0.2985],\n",
       "         [ 0.2258,  0.2309, -0.3508,  0.2601,  0.0544, -0.0075,  0.0578,  0.1150],\n",
       "         [-0.3186, -0.0946,  0.0675,  0.3500, -0.3133,  0.1691, -0.0252, -0.3244],\n",
       "         [ 0.1400, -0.3143, -0.3440, -0.1548, -0.1905,  0.1787, -0.1502, -0.3285],\n",
       "         [ 0.3001,  0.2819, -0.1946, -0.0958,  0.2223,  0.0650, -0.2857, -0.2210],\n",
       "         [ 0.0771,  0.3021, -0.1720, -0.2476,  0.0451,  0.3254, -0.2297, -0.3251]],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:43939032953]>tensor([ 0.1665, -0.2059,  0.0913,  0.1766,  0.1041,  0.3515, -0.0999,  0.2725],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:62193368468]>tensor([[-0.1394,  0.0783,  0.1068,  0.1363, -0.2605,  0.1557,  0.2424,  0.2766],\n",
       "         [-0.1422, -0.0787, -0.3054, -0.1694, -0.0393,  0.0338, -0.0167, -0.1989],\n",
       "         [-0.0570, -0.1110,  0.2078, -0.0234,  0.2164,  0.1868,  0.1133,  0.1054],\n",
       "         [-0.1819,  0.2261, -0.2848,  0.0894, -0.0084,  0.1691, -0.1023, -0.0735],\n",
       "         [ 0.0791,  0.0421,  0.0150,  0.1830,  0.2216,  0.3156,  0.0197,  0.1150],\n",
       "         [ 0.2437, -0.1574,  0.2277, -0.2180, -0.0704,  0.0894,  0.0949, -0.3468],\n",
       "         [ 0.1651,  0.3245, -0.3539, -0.1031, -0.1436,  0.1021, -0.1912,  0.0606],\n",
       "         [ 0.2974, -0.1155,  0.2732, -0.1237, -0.3497,  0.3076, -0.0620,  0.0448]],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:3851057944]>tensor([ 0.1437,  0.2935,  0.1610, -0.0628,  0.2106,  0.3466,  0.2562,  0.3128],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:27553285308]>tensor([[ 0.2708,  0.2490,  0.1730, -0.2806,  0.2587, -0.3234, -0.3478, -0.1436],\n",
       "         [ 0.2793, -0.1804,  0.3007,  0.0400, -0.3511, -0.3423, -0.0221, -0.1078],\n",
       "         [-0.2481, -0.2349, -0.2170, -0.1416,  0.1008,  0.2340, -0.3088, -0.0881],\n",
       "         [ 0.2773, -0.0422, -0.1135, -0.3458,  0.0256, -0.3219, -0.2635,  0.1632],\n",
       "         [ 0.0090,  0.3298,  0.2056, -0.0114,  0.1103, -0.1026,  0.1400,  0.2483],\n",
       "         [ 0.2399, -0.3030, -0.2521, -0.0640, -0.3426, -0.0491, -0.2218, -0.0538],\n",
       "         [-0.0502, -0.0968,  0.0453, -0.1486,  0.3304, -0.1497,  0.1853,  0.2995],\n",
       "         [ 0.2929,  0.1837, -0.0386,  0.3462, -0.3258,  0.1533,  0.0660,  0.3087],\n",
       "         [-0.2908,  0.1654, -0.1458, -0.2363,  0.1253,  0.0027,  0.0326,  0.1652],\n",
       "         [ 0.0591,  0.0590,  0.2651, -0.0394,  0.0772, -0.2347, -0.1973,  0.1698]],\n",
       "        grad_fn=<SubBackward0>),\n",
       " AutogradTensor>PlaceHolder[Id:15283181914]>tensor([-0.1087,  0.3303,  0.0896,  0.2716,  0.0584, -0.2765,  0.0754, -0.0953,\n",
       "          0.3068,  0.1793], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up dummy inputs.\n",
    "# These size must always be the same when using the model.\n",
    "batch_size = 3\n",
    "# Changing the sequence length from 2 to 3 can make the Torchscript take much much longer.\n",
    "# (a few seconds to 1min when testing on a decent machine).\n",
    "# The sequence length should always be the same size when running the Plan.\n",
    "# But maybe we can one day be more dynamic and use it.\n",
    "# For non-example purposes, increase the sequence length and use padding on the data.\n",
    "sequence_length = 2\n",
    "vocab_size = model.vocab_size\n",
    "# Data has the index of the word in a vocabulary.\n",
    "# Start token indices after padding index.\n",
    "token_start_index = max(model.padding_idx + 1, 1)\n",
    "data = th.randint(token_start_index, vocab_size, (sequence_length, batch_size))\n",
    "\n",
    "# Test the model with no default hidden state.\n",
    "output, hidden = model(data)\n",
    "assert output.shape == th.Size([batch_size, vocab_size])\n",
    "assert hidden.shape == th.Size([batch_size, model.rnn.hidden_size])\n",
    "\n",
    "# The model can initialize the hidden state if it is not set\n",
    "# but this might not work within a Plan.\n",
    "initial_hidden = model.init_hidden(batch_size)\n",
    "\n",
    "# Predicting the next word for each sequence.\n",
    "targets = th.randint(0, vocab_size, (batch_size,))\n",
    "targets = nn.functional.one_hot(targets, vocab_size)\n",
    "\n",
    "lr = th.tensor([0.1])\n",
    "batch_size = th.tensor([batch_size])\n",
    "sequence_length = th.tensor([sequence_length])\n",
    "model_state = list(model.parameters())\n",
    "\n",
    "# Build Plan\n",
    "train.build(\n",
    "    data,\n",
    "    initial_hidden,\n",
    "    targets,\n",
    "    lr,\n",
    "    batch_size,\n",
    "    sequence_length,\n",
    "    model_state,\n",
    "    trace_autograd=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.randint(0, vocab_size, (batch_size,)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
