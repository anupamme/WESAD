{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/santteegt/om-fol-timeseries/blob/master/Federated_LSTM_model_Chest_device.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zd_09v8M6m_r",
    "outputId": "55ec9c8e-158d-43ee-d647-dae8a58fba32"
   },
   "outputs": [],
   "source": [
    "# Uncomment in Colab\n",
    "# !pip install syft[udacity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Dbs_QI3A68rP",
    "outputId": "360e08b2-cbec-462e-c712-8728b581b7a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/Users/santteegt/anaconda3/envs/pysyft-dev/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.2.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/santteegt/anaconda3/envs/pysyft-dev/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import syft as sy\n",
    "import sys\n",
    "import torch\n",
    "from torch.nn import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# force protobuf serialization for tensors\n",
    "hook.local_worker.framework = None\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    print('CUDA AVAILABLE')\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "# IMPORTANT: Local worker should not be a client worker\n",
    "# hook.local_worker.is_client_worker = False\n",
    "# server = hook.local_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1G80vgSRvqda"
   },
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 25\n",
    "        self.test_batch_size = 5\n",
    "        self.epochs = 5\n",
    "        self.lr = 0.02\n",
    "        self.seed = 42\n",
    "        self.log_interval = 1 # Log info at each batch\n",
    "        self.precision_fractional = 3\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "torch.manual_seed(args.seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6vfSFx870zZ"
   },
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "YOnm2rgi7uFs",
    "outputId": "40dfabf8-5636-4567-bccb-ca3ad578f7cd"
   },
   "outputs": [],
   "source": [
    "# Uncomment in Colab\n",
    "# !gdown --id 1_56HAZc2XSXQCAR4iZODq9VAoyq5CvAs  # segmented dataset on my Google drive\n",
    "# !ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2ct2yQJq7zwi",
    "outputId": "8b6de2fe-32fa-41c1-bd09-f84072340875"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121813, 79)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./segmented_data/all_subjects.csv', index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJTQqf1E74c_"
   },
   "outputs": [],
   "source": [
    "df.label = df.label - 1  # TODO: This should be done during data segmantation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlOI_Q7P8d1M"
   },
   "source": [
    "### Creating virtual workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjCJusu68h-o"
   },
   "outputs": [],
   "source": [
    "subjects = df['subject'].unique()\n",
    "subjects.sort()\n",
    "\n",
    "workers = [sy.VirtualWorker(hook, id=subject) for subject in subjects]\n",
    "crypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex7OnjPa8bxD"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hbFUGDjTvIA0"
   },
   "outputs": [],
   "source": [
    "# def secret_share(tensor):\n",
    "#         \"\"\"\n",
    "#         Transform to fixed precision and secret share a tensor.\n",
    "#         Useful when performing SMPC\n",
    "#         \"\"\"\n",
    "#         return (\n",
    "#             tensor\n",
    "#             .fix_precision(precision_fractional=precision_fractional)\n",
    "#             .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CqwpShOuGaw"
   },
   "outputs": [],
   "source": [
    "# def one_hot_of(index_tensor, classes=3):\n",
    "#         \"\"\"\n",
    "#         Transform to one hot tensor\n",
    "        \n",
    "#         Example:\n",
    "#             [0, 1, 2]\n",
    "#             =>\n",
    "#             [[1., 0., 0.],\n",
    "#              [0., 1., 0.],\n",
    "#              [0., 0., 1.]]\n",
    "            \n",
    "#         \"\"\"\n",
    "#         onehot_tensor = torch.zeros(*index_tensor.shape, classes)\n",
    "#         onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "#         return onehot_tensor\n",
    "\n",
    "# # one_hot_of(torch.tensor(df['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMb9yRRXie3t"
   },
   "source": [
    "#### Exploring Strategies for train/validation data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIL0mnVhmiom"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split, TimeSeriesSplit, GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqrBwftIWKaP"
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10, xlim=[0, 100]):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['class', 'group']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=xlim)\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "LLp8LJjUWJm6",
    "outputId": "78f1e7b0-359e-415b-ef53-9412c3c31c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7900, 79)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_splits = 3\n",
    "subject = 'S3'\n",
    "\n",
    "data = df[df['subject'] == subject].reset_index(drop=True)\n",
    "print(data.shape)\n",
    "X = data.drop(columns=['subject', 'label']).copy()\n",
    "y = data['label'].to_numpy()\n",
    "\n",
    "# groups = y\n",
    "subgroups = [int(perc * group) \n",
    "            for group in data[['label', 'subject']].groupby('label', sort=False).count().reset_index()['subject'].to_list() \n",
    "            for perc in [0.7, 0.3] ]\n",
    "groups = np.hstack([[i] * sg for i, sg in enumerate(subgroups)])\n",
    "\n",
    "\n",
    "cvs = [TimeSeriesSplit(n_splits), StratifiedKFold(n_splits, shuffle=False)]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(cvs), figsize=(15, 5))\n",
    "\n",
    "for i, cv in enumerate(cvs):\n",
    "    ax = axs[i]\n",
    "    plot_cv_indices(cv, X, y, groups, ax, n_splits, xlim=[0, X.shape[0]])\n",
    "    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "              ['Val set', 'Train set'], loc=(1.02, .8))\n",
    "    # Make the legend fit\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zxLrO1-cWJYF",
    "outputId": "5daf6cd1-2b72-44ac-cbbe-809c707679f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold to choose 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5267,), (2633,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits = 3\n",
    "cv = StratifiedKFold(n_splits, shuffle=False)\n",
    "folds = list(cv.split(X=X, y=y, groups=groups))\n",
    "\n",
    "random_choice = randint(0, n_splits - 1)\n",
    "print('Fold to choose', random_choice)\n",
    "train, val = folds[random_choice]\n",
    "train.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b4cCbiKmkfMq",
    "outputId": "31f57b3c-c897-4876-ca78-ab18ee9fe514"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2633, 79)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[val].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0ITMsoizbvF"
   },
   "outputs": [],
   "source": [
    "# class WESADDataset(torch.utils.data.Dataset):\n",
    "#     #Constructor is mandatory\n",
    "#     def __init__(self, dataframe, transform=None):\n",
    "#         # normalizer = StandardScaler()\n",
    "#         # Since different subjects have different responses on the signal values, min-max normalisation was conducted across different signals to normalize them to the same scale\n",
    "#         normalizer = MinMaxScaler()\n",
    "#         self.dataframe = dataframe.drop(columns=['subject','label'])\n",
    "#         # self.X = self.dataframe.astype(np.float32).to_numpy()\n",
    "#         # Normalize features\n",
    "#         self.X = torch.tensor(normalizer.fit_transform(self.dataframe.astype(np.float32))).tag('#wesad', '#chest_device', '#data')\n",
    "#         self.labels = torch.tensor(dataframe['label'].to_numpy()).tag('#wesad', '#chest_device', '#target')\n",
    "#         self.transform = transform # e.g. torch.Tensor\n",
    "    \n",
    "#     # def to_torchtensor(self):            \n",
    "#     #     self.dataframe = torch.from_numpy(self.dataframe)\n",
    "#     #     self.labels = torch.from_numpy(self.labels)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         #Mandatory\n",
    "#         '''Returns:\n",
    "#                 Length [int]: Length of Dataset/batches\n",
    "#         '''\n",
    "#         return self.dataframe.shape[0]\n",
    "\n",
    "#     def __getitem__(self, idx): \n",
    "#         #Mandatory \n",
    "        \n",
    "#         '''Returns:\n",
    "#                     Data [Torch Tensor]: \n",
    "#                     Target [ Torch Tensor]:\n",
    "#         '''\n",
    "#         sample = self.X[idx]\n",
    "#         # sample = self.X[idx].reshape(1, -1)\n",
    "#         target = self.labels[idx]\n",
    "                \n",
    "#         if self.transform:\n",
    "#             sample = self.transform(sample)\n",
    "\n",
    "#         return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZgV4W0iq3-Q"
   },
   "outputs": [],
   "source": [
    "def get_data_loader(df, subject, train_batch_size=25, val_batch_size=5, random_state=42):\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    data = df[df['subject'] == subject].reset_index(drop=True)\n",
    "\n",
    "    X = data.drop(columns=['subject', 'label']).copy()\n",
    "    X = normalizer.fit_transform(X.astype(np.float32))\n",
    "    y = data['label'].to_numpy()\n",
    "\n",
    "    # TODO: what's the best strategy to get train/val for each worker?\n",
    "    #       Right now validation set contains data from one affect state only\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "    # # train = WESADDataset(train_data)\n",
    "    # # test = WESADDataset(val_data)\n",
    "\n",
    "    # X_train = torch.from_numpy(X_train)\n",
    "    # y_train = one_hot_of(torch.from_numpy(y_train))\n",
    "    # X_val = torch.from_numpy(X_val)\n",
    "    # y_val = one_hot_of(torch.from_numpy(y_val))\n",
    "\n",
    "    # train = TensorDataset(X_train, y_train)\n",
    "    # test = TensorDataset(X_val, y_val)\n",
    "\n",
    "    \n",
    "\n",
    "    # Train/Validation split strategy\n",
    "    # #########################\n",
    "    n_splits = 3\n",
    "    subgroups = [round(perc * group) \n",
    "                for group in data[['label', 'subject']].groupby('label', sort=False).count().reset_index()['subject'].to_list() \n",
    "                for perc in [0.7, 0.3] ]\n",
    "    groups = np.hstack([[i] * sg for i, sg in enumerate(subgroups)])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits, shuffle=False)\n",
    "    # print('Sizes', len(X), len(y), len(groups))\n",
    "    folds = list(cv.split(X=X, y=y, groups=groups))\n",
    "\n",
    "    random_choice = randint(0, n_splits - 1)\n",
    "    print('Fold to choose', subject, random_choice)\n",
    "    train_idx, val_idx = folds[random_choice]\n",
    "    \n",
    "    # #########################\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    y_train = y[train_idx]\n",
    "#     y_train = one_hot_of(torch.from_numpy(y_train))  # one-hot encoder\n",
    "    y_train = nn.functional.one_hot(torch.from_numpy(y_train), 3)\n",
    "    \n",
    "#     X_train = X_train.fix_precision(precision_fractional=3)\n",
    "#     y_train = y_train.fix_precision(precision_fractional=3)\n",
    "\n",
    "#     train = TensorDataset(X_train, y_train)\n",
    "    train_ds = sy.BaseDataset(X_train, y_train)\n",
    "\n",
    "    valid = data.iloc[val_idx].copy()\n",
    "\n",
    "\n",
    "#     train_dl = torch.utils.data.DataLoader(train, \n",
    "#                                            batch_size=train_batch_size, \n",
    "#                                            shuffle=False, \n",
    "# #                                            pin_memory=True, \n",
    "#                                            num_workers=4\n",
    "#                                           )\n",
    "\n",
    "    val_dl = torch.utils.data.DataLoader(valid, \n",
    "                                         batch_size=val_batch_size, \n",
    "                                         shuffle=False, \n",
    "#                                          pin_memory=True, \n",
    "                                         num_workers=4\n",
    "                                        )\n",
    "    \n",
    "#     return train_dl, val_dl\n",
    "    return train_ds, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XD1MSJdZ2MI2"
   },
   "outputs": [],
   "source": [
    "def get_private_data_loaders(workers, crypto_provider=None, precision_fractional=None):\n",
    "    train_dataloaders = []\n",
    "    # private_val_loader = []\n",
    "    val_dataloaders = []\n",
    "    for worker in workers:\n",
    "        # train_dl, val_dl = get_data_loader(df, worker.id)\n",
    "        train_ds, val_dl = get_data_loader(df, worker.id)\n",
    "        train_dataloaders.append(train_ds.tag('#wesad', '#chest_device', '#data').send(worker))\n",
    "#         for data_, labels_ in train_dl:\n",
    "#             data = data_.tag('#wesad', '#chest_device', '#data').send(worker)\n",
    "#             targets = labels_.tag('#wesad', '#chest_device', '#target').send(worker)\n",
    "#             train_dataloaders.append(BaseDataset(data, targets, owner=worker))\n",
    "            \n",
    "            \n",
    "#             private_train_loaders.append((data.tag('#wesad', '#chest_device', '#data').send(worker), \n",
    "#                                          labels.tag('#wesad', '#chest_device', '#target').send(worker)))\n",
    "            # private_train_loader.append((data.tag('#wesad', '#chest_device', '#data').fix_precision(precision_fractional=args.precision_fractional).send(worker), \n",
    "            #                              labels.tag('#wesad', '#chest_device', '#target').fix_precision(precision_fractional=args.precision_fractional).send(worker)))\n",
    "        # for data, labels in val_dl:\n",
    "        #     private_val_loader.append((data.tag('#wesad', '#chest_device', '#data').send(worker), \n",
    "        #                                labels.tag('#wesad', '#chest_device', '#target').send(worker)))\n",
    "        #     # private_val_loader.append((data.tag('#wesad', '#chest_device', '#data').fix_precision(precision_fractional=args.precision_fractional).send(worker), \n",
    "        #     #                            labels.tag('#wesad', '#chest_device', '#target').fix_precision(precision_fractional=args.precision_fractional).send(worker)))\n",
    "        # private_val_loader.append(val_dl)\n",
    "        \n",
    "        val_dataloaders.append(val_dl)\n",
    "    return train_dataloaders, val_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "KDDh4tZ32iIk",
    "outputId": "130f1397-77df-4fde-b3f0-4a6de6591b72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold to choose S10 1\n",
      "Fold to choose S11 0\n",
      "Fold to choose S13 0\n",
      "Fold to choose S14 2\n",
      "Fold to choose S15 2\n",
      "Fold to choose S16 2\n",
      "Fold to choose S17 1\n",
      "Fold to choose S2 0\n",
      "Fold to choose S3 1\n",
      "Fold to choose S4 1\n",
      "Fold to choose S5 1\n",
      "Fold to choose S6 1\n",
      "Fold to choose S7 1\n",
      "Fold to choose S8 1\n",
      "Fold to choose S9 0\n",
      "15 15\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "precision_fractional = 3\n",
    "\n",
    "private_ds, central_val_dl = get_private_data_loaders(workers=workers,\n",
    "#                                                       crypto_provider=crypto_provider,\n",
    "#                                                       precision_fractional=precision_fractional,\n",
    "                                                     )\n",
    "\n",
    "print(len(private_ds), len(central_val_dl))\n",
    "federated_train_ds = sy.FederatedDataset(private_ds)\n",
    "federated_train_dl = sy.FederatedDataLoader(federated_train_ds, shuffle=False, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gYd02AGu6QDe",
    "outputId": "9ae52f2f-af5a-4c0b-80fd-5c3a0422df2c"
   },
   "outputs": [],
   "source": [
    "# w = workers[1]\n",
    "# len(list(w._objects.keys()))\n",
    "# x = w._objects[list(w._objects.keys())[0]]\n",
    "# print(x)\n",
    "# len(w.search(['#wesad', '#chest_device', '#target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6_0hRslktwP"
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# from syft.frameworks.torch.nn import LSTM\n",
    "# from handcrafted_RNN import LSTM\n",
    "from custom_LSTM import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_0JHd7aHN-K"
   },
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_NJSvnIutDj"
   },
   "outputs": [],
   "source": [
    "class WesadLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=3, lstm_layers=1, dropout=0.2):\n",
    "        # super(WesadLSTM, self).__init__(id=\"encrypted-model\")\n",
    "        super(WesadLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.classes = output_dim\n",
    "        self.lstm = LSTM(input_size=input_dim, hidden_size=input_dim, num_layers=lstm_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return self.lstm.init_hidden(batch_size), self.lstm.init_hidden(batch_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        lstm_out, _ = self.lstm(x, h)\n",
    "#         out = self.fc(lstm_out.view(-1, self.hidden_dim))\n",
    "        out = self.fc(lstm_out)\n",
    "#         out = F.softmax(out.view(-1, self.classes), dim=1)\n",
    "#         out = F.softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_params(module, params_list, start_param_idx=0):\n",
    "    \"\"\"\n",
    "    Set params list into model recursively.\n",
    "    \"\"\"\n",
    "    param_idx = start_param_idx\n",
    "\n",
    "    for name, param in module._parameters.items():\n",
    "        # A param can be None if it is not trainable.\n",
    "        if param is not None:\n",
    "            module._parameters[name] = params_list[param_idx]\n",
    "            param_idx += 1\n",
    "\n",
    "    for name, child in module._modules.items():\n",
    "        if child is not None:\n",
    "            param_idx = set_model_params(child, params_list, param_idx)\n",
    "\n",
    "    return param_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_sgd(param, **kwargs):\n",
    "    print(param)\n",
    "    print(kwargs['lr'])\n",
    "    print(param.requires_grad, param.grad)\n",
    "    return param - kwargs['lr'] * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_with_logits(logits, targets, batch_size):\n",
    "    \"\"\" Calculates softmax entropy\n",
    "        Args:\n",
    "            * logits: (NxC) outputs of dense layer\n",
    "            * targets: (NxC) one-hot encoded labels\n",
    "            * batch_size: value of N, temporarily required because Plan cannot trace .shape\n",
    "    \"\"\"\n",
    "    # numstable logsoftmax\n",
    "    norm_logits = logits - logits.max()\n",
    "    log_probs = norm_logits - norm_logits.exp().sum(dim=1, keepdim=True).log()\n",
    "    # NLL, reduction = mean\n",
    "    return -(targets * log_probs).sum() / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 25\n",
    "val_batch_size = 5\n",
    "input_dim = 77\n",
    "output_dim = 3\n",
    "lstm_layers = 5\n",
    "dropout = 0.5\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Plan definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(77, 392)\n",
    "        self.fc2 = nn.Linear(392, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFUU0_KNJCK0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = WesadLSTM(input_dim=input_dim, hidden_dim=input_dim, output_dim=output_dim, lstm_layers=lstm_layers,\n",
    "                  dropout=dropout)\n",
    "\n",
    "# model = Net()\n",
    "\n",
    "@sy.func2plan()\n",
    "def training_plan(data, target, h, c, batch_size, lr, model_parameters):\n",
    "    # inject params into model\n",
    "    set_model_params(model, model_parameters)\n",
    "    # forward pass\n",
    "    logits = model(data, (h, c))\n",
    "#     logits = model.forward(data)\n",
    "    \n",
    "#     batch_size = out.shape[0]\n",
    "#     loss = ((out - target)**2).sum().refresh()/batch_size\n",
    "    # loss\n",
    "    loss = softmax_cross_entropy_with_logits(logits, target, batch_size)\n",
    "    print('loss', loss)\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    print('post backprop')\n",
    "    \n",
    "    # step\n",
    "    updated_params = [\n",
    "        naive_sgd(param, lr=lr)\n",
    "        for param in model_parameters\n",
    "    ]\n",
    "    \n",
    "    # accuracy\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    y = torch.argmax(target, dim=1)\n",
    "    acc = pred.eq(y).sum().float() / batch_size\n",
    "\n",
    "    return (\n",
    "        loss,\n",
    "        acc,\n",
    "        *updated_params\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss AutogradTensor>PlaceHolder[Id:27806206920]>tensor([1.0940])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fc01348ff398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0mmodel_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                         trace_autograd=True)\n\u001b[0m",
      "\u001b[0;32m~/openmined/PySyft/syft/execution/plan.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, trace_autograd, *args)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# print('Args', args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# print('Framework args', framework_kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mframework_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# Register inputs in role\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f4783bb04c20>\u001b[0m in \u001b[0;36mtraining_plan\u001b[0;34m(data, target, h, c, batch_size, lr, model_parameters)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post backprop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# And chain structure than self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mbackwards_grad\u001b[0;34m(grad_fn, in_grad)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;34m\"to see if it's missing.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         )\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mback_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mbackwards_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_grad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/gradients_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/generic/frameworks/hook/tensors.py\u001b[0m in \u001b[0;36mtracing_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcreate_tracing_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mtracing_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                 \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_syft_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_inplace_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openmined/PySyft/syft/frameworks/torch/torch_attributes.py\u001b[0m in \u001b[0;36mis_inplace_method\u001b[0;34m(self, method_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace_methods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mis_inplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inplace_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = torch.rand((train_batch_size, input_dim), dtype=torch.float32)\n",
    "\n",
    "target = torch.randint(0, output_dim - 1, (train_batch_size,))\n",
    "target = nn.functional.one_hot(target, output_dim)\n",
    "\n",
    "h, c = model.init_hidden(train_batch_size)\n",
    "\n",
    "# model_state = list(model.parameters())\n",
    "model_state = [param.data for param in model.parameters()] # raw tensors instead of nn.Parameter\n",
    "\n",
    "\n",
    "# Plan._build_translators = []\n",
    "training_plan.build(data, target, h, c, \n",
    "                        torch.tensor([train_batch_size]), \n",
    "                        torch.tensor([lr]), \n",
    "                        model_state, \n",
    "                        trace_autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FAIL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d6ce9cbc66c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# END - Skip cells below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mFAIL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mON\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mPURPOSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'FAIL' is not defined"
     ]
    }
   ],
   "source": [
    "# END - Skip cells below\n",
    "FAIL-ON-PURPOSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous Federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfM1fLNGtOqT"
   },
   "outputs": [],
   "source": [
    "train_batch_size = args.batch_size\n",
    "val_batch_size = args.test_batch_size\n",
    "input_dim = df.drop(columns=['subject', 'label']).shape[1]\n",
    "output_dim = 3\n",
    "# output_dim = 1\n",
    "lstm_layers = 5\n",
    "dropout = 0.5\n",
    "lr = 1e-4\n",
    "model = WesadLSTM(input_dim=input_dim, hidden_dim=input_dim, output_dim=output_dim, lstm_layers=lstm_layers,\n",
    "                  dropout=dropout)\n",
    "# model.fix_precision()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr);\n",
    "# optimizer.fix_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = train_batch_size\n",
    "VAL_BATCH_SIZE = val_batch_size\n",
    "HIDDEN_DIM = input_dim\n",
    "\n",
    "log_interval = 100  # batches\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    \n",
    "    ######### Training ##########\n",
    "    \n",
    "    losses = []\n",
    "    # Batch loop\n",
    "    last_worker = None\n",
    "    for batch_idx, (inputs, labels) in enumerate(federated_train_dl):\n",
    "        # Location of current batch\n",
    "        worker = inputs.location\n",
    "        if last_worker != worker.id:\n",
    "            last_worker = worker.id\n",
    "            print('\\nfrom', last_worker, end=\"\")\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "            \n",
    "        batch_size = inputs.shape[0]\n",
    "            \n",
    "        # Initialize hidden state and send it to worker\n",
    "        hc = (torch.Tensor(np.zeros((batch_size, HIDDEN_DIM))).send(worker), torch.Tensor(np.zeros((batch_size, HIDDEN_DIM))).send(worker))\n",
    "        # Send model to current worker\n",
    "        model.send(worker)\n",
    "        # Setting accumulated gradients to zero before backward step\n",
    "        optimizer.zero_grad()\n",
    "        # Output from the model\n",
    "        output, _ = model(inputs, hc)\n",
    "        # Calculate the loss and perform backprop\n",
    "        # loss = criterion(output.squeeze(), labels.float())\n",
    "        batch_size = output.shape[0]\n",
    "#         print('sum', ((output - labels)**2).sum().get())\n",
    "#         loss = ((output - labels)**2).sum().refresh() / batch_size\n",
    "        loss = ((output - labels)**2).sum() / batch_size\n",
    "        loss.backward()\n",
    "        # Clipping the gradient to avoid explosion\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        # Backpropagation step\n",
    "        optimizer.step() \n",
    "        # Get the model back to the local worker\n",
    "        model.get()\n",
    "        losses.append(loss.get())\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"\\nEpoch {}/{}...  \\\n",
    "            Batches: {} ... \\\n",
    "            Training loss: {:.5f}\".format(e+1, EPOCHS, batch_idx, sum(losses)/len(losses)))  \n",
    "            \n",
    "        break\n",
    "    \n",
    "    ######## Evaluation ##########\n",
    "    \n",
    "    # Model in evaluation mode\n",
    "    print('now eval')\n",
    "#     model.eval()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         test_preds = []\n",
    "#         test_labels_list = []\n",
    "#         eval_losses = []\n",
    "\n",
    "#         for inputs, labels in central_val_dl:\n",
    "#             # get current location\n",
    "#             worker = inputs.location\n",
    "            \n",
    "#             # Initialize hidden state and send it to worker\n",
    "#             # h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
    "#             hc = (torch.Tensor(np.zeros((VAL_BATCH_SIZE, HIDDEN_DIM))), torch.Tensor(np.zeros((VAL_BATCH_SIZE, HIDDEN_DIM))))\n",
    "#             # Send model to worker\n",
    "#             # model.send(worker)\n",
    "            \n",
    "#             output, _ = model(inputs, hc)\n",
    "# #             print('output_test', output.get())\n",
    "# #             loss = criterion(output.squeeze(), labels.float())\n",
    "#             batch_size = output.shape[0]\n",
    "#             loss = ((output - labels)**2).sum() / batch_size\n",
    "#             eval_losses.append(loss.get())\n",
    "#             preds = output.squeeze().get()\n",
    "#             test_preds += list(preds.numpy())\n",
    "#             test_labels_list += list(labels.get().numpy().astype(int))\n",
    "#             # Get the model back to the local worker\n",
    "#             # model.get()\n",
    "        \n",
    "#         # score = roc_auc_score(test_labels_list, test_preds)\n",
    "    \n",
    "# #     print(\"Epoch {}/{}...  \\\n",
    "# #     AUC: {:.3%}...  \\\n",
    "# #     Training loss: {:.5f}...  \\\n",
    "# #     Validation loss: {:.5f}\".format(e+1, EPOCHS, score, sum(losses)/len(losses), sum(eval_losses)/len(eval_losses)))\n",
    "#     print(\"Epoch {}/{}...  \\\n",
    "#     Training loss: {:.5f}...  \\\n",
    "#     Validation loss: {:.5f}\".format(e+1, EPOCHS, sum(losses)/len(losses), sum(eval_losses)/len(eval_losses)))\n",
    "    \n",
    "#     model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP (until Plan is fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "colab_type": "code",
    "id": "5Avz3QDnCbDR",
    "outputId": "10a64f6b-2a2f-49f7-871e-76b4f0f0c8a9"
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "for data, labels in private_train_loader:\n",
    "    optimizer.zero_grad()\n",
    "#     set_model_params(model, list(model.parameters()))\n",
    "    try_batch.build(data, labels, trace_autograd=True)\n",
    "#     try_batch.build(data)\n",
    "    try_batch(model, private_train_loader)\n",
    "    \n",
    "    optimizer.step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltmICz63HRqr"
   },
   "source": [
    "### Train/Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yN50zft-458A"
   },
   "outputs": [],
   "source": [
    "## For Federated learning with secure aggregation\n",
    "# def update_model(data, target, model, optimizer):\n",
    "#     model.send(data.location)\n",
    "#     optimizer.zero_grad()\n",
    "#     pred = model(data)\n",
    "#     loss = F.mse_loss(pred.view(-1), target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8Wi-5tQHVFh"
   },
   "outputs": [],
   "source": [
    "# def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "#         model.send(data.location) # <-- NEW: send the model to the right location\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = F.nll_loss(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         model.get() # <-- NEW: get the model back\n",
    "#         if batch_idx % args.log_interval == 0:\n",
    "#             loss = loss.get() # <-- NEW: get the loss back\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "#                 100. * batch_idx / len(federated_train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def train(args, model, federated_train_loader, optimizer, epoch, device=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(federated_train_loader):\n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        print(data.shape)\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "            \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * data.shape[0], len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n",
    "\n",
    "\n",
    "## Federated learning with secure aggregation\n",
    "# def train(epoch):\n",
    "#     for data_index in range(len(remote_dataset[0])-1):\n",
    "#         # update remote models\n",
    "#         for remote_index in range(len(compute_nodes)):\n",
    "#             data, target = remote_dataset[remote_index][data_index]\n",
    "#             models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "\n",
    "#         # encrypted aggregation\n",
    "#         new_params = list()\n",
    "#         for param_i in range(len(params[0])):\n",
    "#             spdz_params = list()\n",
    "#             for remote_index in range(len(compute_nodes)):\n",
    "#                 spdz_params.append(params[remote_index][param_i].fix_precision().share(bob, alice, crypto_provider=james).get())\n",
    "\n",
    "#             new_param = (spdz_params[0] + spdz_params[1]).get().float_precision()/2\n",
    "#             new_params.append(new_param)\n",
    "\n",
    "#         # cleanup\n",
    "#         with torch.no_grad():\n",
    "#             for model in params:\n",
    "#                 for param in model:\n",
    "#                     param *= 0\n",
    "\n",
    "#             for model in models:\n",
    "#                 model.get()\n",
    "\n",
    "#             for remote_index in range(len(compute_nodes)):\n",
    "#                 for param_index in range(len(params[remote_index])):\n",
    "#                     params[remote_index][param_index].set_(new_params[param_index])\n",
    "\n",
    "## When sharing the model among all participants (SMPC)\n",
    "# def train(args, model, private_train_loader, optimizer, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         output = model(data)\n",
    "        \n",
    "#         # loss = F.nll_loss(output, target)  <-- not possible here\n",
    "#         batch_size = output.shape[0]\n",
    "#         loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "        \n",
    "#         loss.backward()\n",
    "        \n",
    "#         optimizer.step()\n",
    "\n",
    "#         if batch_idx % args.log_interval == 0:\n",
    "#             loss = loss.get().float_precision()\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "#                 epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "#                 100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DUEZkVsHZTx"
   },
   "outputs": [],
   "source": [
    "# def test(args, model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "#             pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def test(args, model, private_test_loader, device=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    total_rec = 0\n",
    "    for data, target in private_test_loader:\n",
    "        total_rec += len(data)\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    # test_loss /= len(test_loader.dataset)\n",
    "    test_loss /= total_rec\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n",
    "\n",
    "\n",
    "## Federated learning with secure aggregation\n",
    "# def test():\n",
    "#     models[0].eval()\n",
    "#     test_loss = 0\n",
    "#     for data, target in test_loader:\n",
    "#         output = models[0](data)\n",
    "#         test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "#         pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "#     print('Test set: Average loss: {:.4f}\\n'.format(test_loss))\n",
    "\n",
    "\n",
    "## When sharing the model among all participants (SMPC)\n",
    "# def test(args, model, private_test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in private_test_loader:\n",
    "#             start_time = time.time()\n",
    "            \n",
    "#             output = model(data)\n",
    "#             pred = output.argmax(dim=1)\n",
    "#             correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "#     correct = correct.get().float_precision()\n",
    "#     print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         correct.item(), len(private_test_loader)* args.test_batch_size,\n",
    "#         100. * correct.item() / (len(private_test_loader) * args.test_batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtAN73TBHc6o"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzEQkZGlHZ2_"
   },
   "outputs": [],
   "source": [
    "## Federated learning with secure aggregation\n",
    "# model = Net()\n",
    "# model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "# optimizer = optimizer.fix_precision() \n",
    "\n",
    "# for epoch in range(1, args.epochs + 1):\n",
    "#     train(args, model, private_train_loader, optimizer, epoch)\n",
    "#     test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bb6OTTd4AAtq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "9kKkx458_htC",
    "outputId": "c2117ce9-730a-4f6e-9bb8-88f554d3dd31"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(args, model, private_train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4JQlQypBYbq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNgtXM6iwmFPzwWpz+eUpOb",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Federated LSTM model - Chest device.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
